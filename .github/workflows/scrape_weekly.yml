name: Weekly Coffee Review Scraper

on:
  schedule:
    # Run every Sunday at 6 AM UTC
    - cron: '0 6 * * 0'
  workflow_dispatch: # Allow manual trigger

jobs:
  scrape-new-reviews:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Safety limit
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r data_pipeline/requirements.txt
        
    - name: Fetch Latest Sitemap
      run: python data_pipeline/scripts/fetch_sitemap.py
      
    - name: Scrape New Reviews Only
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      run: |
        # Only scrape reviews not already in database
        python data_pipeline/scripts/scrape_and_embed.py --limit 200 --skip-existing

    - name: Populate Normalized Columns
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      run: |
        # Populate country, price_usd, review_year, roast_category for new entries
        python data_pipeline/scripts/migrate_clean.py
